import cloudscraper
from lxml import html
import re
import os
import json
from datetime import datetime
import time
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed

def extract_episode_data(ep_link, scraper, safe_title, anime_title, existing_titles):
    try:
        print(f"ğŸ”— Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ù„Ù‚Ø©: {ep_link}")
        ep_resp = scraper.get(ep_link)
        ep_resp.raise_for_status()
        ep_tree = html.fromstring(ep_resp.content)

        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ØµÙØ­Ø©
        title_elements = ep_tree.xpath('/html/body/div[3]/div/h3/text()')
        full_title = title_elements[0].strip() if title_elements else "Ø­Ù„Ù‚Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©"

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆØ¹ Ø§Ù„Ø­Ù„Ù‚Ø© ÙˆØ±Ù‚Ù…Ù‡Ø§
        match = re.search(r"(Ø§Ù„Ø­Ù„Ù‚Ø©(?:\s+Ø§Ù„Ø®Ø§ØµØ©)?|Ø§Ù„ÙÙŠÙ„Ù…|ÙÙŠÙ„Ù…)\s*(\d+)", full_title, re.IGNORECASE)
        if match:
            ep_type = match.group(1).strip()
            episode_number = int(match.group(2))
            episode_title = f"{ep_type} {episode_number}"
        else:
            episode_number = None
            episode_title = full_title

        if episode_title in existing_titles:
            print(f"âœ… Ø§Ù„Ø­Ù„Ù‚Ø© '{episode_title}' Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ ØªØ®Ø·ÙŠ.")
            return None

        # Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª
        servers = []
        server_lis = ep_tree.xpath('//*[@id="episode-servers"]/li')
        for li in server_lis:
            try:
                server_name = li.xpath('.//a/text()')[0].strip()
                url = li.xpath('.//a/@data-ep-url')
                url = url[0] if url else li.xpath('.//a/@href')[0]
                if url.startswith("//"):
                    url = "https:" + url
                servers.append({
                    "serverName": server_name,
                    "url": url
                })
            except Exception:
                continue
        episode_number = int(re.search(r'\d+', episode_title).group()) if re.search(r'\d+', episode_title) else None

        # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        ep_data = {
            "number": episode_number,
            "title": episode_title,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "link": f"https://abdo12249.github.io/1/test1/Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ù‡.html?id={safe_title}&episode={episode_number}",
            "image": f"https://abdo12249.github.io/1/images/{safe_title}.webp",
            "servers": servers
        }


        print(f"â• ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬: {episode_title}")
        return ep_data

    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù„Ù‚Ø© Ù…Ù† {ep_link} Ø¨Ø³Ø¨Ø¨: {e}")
        return None


def extract_base_title(raw_title):
    # Ø¥Ø²Ø§Ù„Ø© " - Ø§Ù„Ø­Ù„Ù‚Ø© 1" Ø£Ùˆ " - ÙÙŠÙ„Ù…" Ù…Ù† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    return re.sub(r'\s*[-â€“]\s*(Ø§Ù„Ø­Ù„Ù‚Ø©|Ø§Ù„ÙÙŠÙ„Ù…|ÙÙŠÙ„Ù…)\s*\d*$', '', raw_title.strip(), flags=re.IGNORECASE)


def scrape_single_anime(base_url):
    print(f"\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù†Ù…ÙŠ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·: {base_url}")
    scraper = cloudscraper.create_scraper()

    try:
        response = scraper.get(base_url)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {e}")
        return

    tree = html.fromstring(response.content)
    episode_links = tree.xpath('//*[@id="ULEpisodesList"]/li/a/@href')
    if not episode_links:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø§Øª.")
        return

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£Ù†Ù…ÙŠ
    title_elements = tree.xpath('/html/body/div[3]/div/h3/text()')
    raw_title = title_elements[0].strip() if title_elements else "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
    anime_title = extract_base_title(raw_title)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù†
    cleaned_title = re.sub(r"Ù…Ø¯Ø¨Ù„Ø¬Ø©\s*Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©", "", anime_title, flags=re.IGNORECASE).strip()
    safe_title = re.sub(r'[\\/:*?"<>|]', "", cleaned_title)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø­Ø¸ÙˆØ±Ø© ÙÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª
    safe_title = safe_title.replace(" ", "-").lower()        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨Ø´Ø±Ø·Ø§Øª ÙˆØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©
    safe_title = re.sub(r'[â€“â€”]', '-', safe_title)            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø´Ø±Ø·Ø© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¨Ø´Ø±Ø·Ø© Ø¹Ø§Ø¯ÙŠØ©
    safe_title = re.sub(r'-?(Ø§Ù„Ø­Ù„Ù‚Ø©|ÙÙŠÙ„Ù…|Ø§ÙˆÙØ§)-?\d*', '', safe_title)  # Ø¥Ø²Ø§Ù„Ø© "Ø§Ù„Ø­Ù„Ù‚Ø©-1" Ø£Ùˆ "ÙÙŠÙ„Ù…-1" Ø£Ùˆ "Ø§ÙˆÙØ§-3"
    safe_title = re.sub(r'-+', '-', safe_title).strip('-')   # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø±Ø·Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØ¥Ø²Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©/Ø§Ù„Ù†Ù‡Ø§ÙŠØ©



    # Ø­ÙØ¸ Ø¨Ø§Ø³Ù… Ø«Ø§Ø¨Øª
    filename = f"{safe_title}.json"
    os.makedirs("episodes", exist_ok=True)
    full_path = os.path.join("episodes", filename)

    all_episodes = []
    existing_titles = set()

    if os.path.exists(full_path) and os.path.getsize(full_path) > 0:
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict) and "episodes" in data:
                    all_episodes = data["episodes"]
                    existing_titles = {ep["title"] for ep in all_episodes}
        except Exception:
            pass

    with ThreadPoolExecutor(max_workers=30) as executor:
        future_to_link = {
            executor.submit(
                extract_episode_data,
                link,
                scraper,
                safe_title,
                anime_title,
                existing_titles
            ): link
            for link in episode_links
        }

        for future in as_completed(future_to_link):
            episode = future.result()
            if episode:
                all_episodes.append(episode)

    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø±Ù‚Ù… Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯
    def ep_sort(ep):
        return ep["number"] if isinstance(ep.get("number"), int) else 9999

    all_episodes.sort(key=ep_sort)

    result = {
        "animeTitle": anime_title,
        "episodes": all_episodes
    }

    with open(full_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… ØªÙ… Ø­ÙØ¸ ÙƒÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª ÙÙŠ: {filename}")


def scrape_from_json_file(json_path):
    if not os.path.exists(json_path):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {json_path}")
        return

    with open(json_path, "r", encoding="utf-8") as f:
        try:
            anime_links = json.load(f)
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù JSON: {e}")
            return

    if not isinstance(anime_links, list):
        print("âŒ Ù…Ù„Ù JSON ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª.")
        return

    print(f"ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ù…Ù† {len(anime_links)} Ø£Ù†Ù…ÙŠ...")
    for i, link in enumerate(anime_links, start=1):
        print(f"\nğŸŒŸ Ø£Ù†Ù…ÙŠ Ø±Ù‚Ù… {i}: {link}")
        scrape_single_anime(link)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("â— Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ…Ø±ÙŠØ± Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª.")
        print("Ù…Ø«Ø§Ù„:\npython script.py animes.json")
        sys.exit(1)

    json_file_path = sys.argv[1]
    scrape_from_json_file(json_file_path)
