import cloudscraper
from bs4 import BeautifulSoup
import time
import json
import os
from urllib.parse import urljoin
import concurrent.futures

# ===== Ø¥Ø¹Ø¯Ø§Ø¯ CloudScraper =====
scraper = cloudscraper.create_scraper()

# ===== Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ù†Ù…ÙŠ =====
def collect_anime_links(start_url):
    anime_links = []

    # ØªØ­Ù…ÙŠÙ„ Ø±ÙˆØ§Ø¨Ø· Ù…Ø­ÙÙˆØ¸Ø© Ø³Ø§Ø¨Ù‚Ù‹Ø§ (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)
    if os.path.exists("anime_links_all_pages.json"):
        with open("anime_links_all_pages.json", "r", encoding="utf-8") as f:
            anime_links = json.load(f)

    seen = set(anime_links)
    page_number = 1

    while True:
        print(f"ğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø© {page_number}...")

        url = f"{start_url}page/{page_number}/"
        response = scraper.get(url)
        if response.status_code != 200:
            print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„ØµÙØ­Ø© {page_number}. Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØµÙØ­.")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        cards = soup.select("div.anime-card-container h3 a")
        if not cards:
            print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©.")
            break

        new_found = 0
        for card in cards:
            link = card.get("href", "").strip()
            if link and link not in seen:
                seen.add(link)
                anime_links.append(link)
                new_found += 1

                # Ø­ÙØ¸ ÙÙˆØ±ÙŠ Ù„ÙƒÙ„ Ø±Ø§Ø¨Ø· Ø¬Ø¯ÙŠØ¯
                with open("anime_links_all_pages.json", "w", encoding="utf-8") as f:
                    json.dump(anime_links, f, ensure_ascii=False, indent=2)

        print(f"â• ØªÙ… Ø¥Ø¶Ø§ÙØ© {new_found} Ø±ÙˆØ§Ø¨Ø· Ø¬Ø¯ÙŠØ¯Ø©")
        page_number += 1
        time.sleep(1)

    anime_links = list(dict.fromkeys(anime_links))

    with open("anime_links.json", "w", encoding="utf-8") as f:
        json.dump(anime_links, f, ensure_ascii=False, indent=2)

    return anime_links

# ===== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ Ø­Ù„Ù‚Ø© Ù…Ù† ÙƒÙ„ Ø£Ù†Ù…ÙŠ =====
def extract_first_episode(anime_url, already_done):
    if anime_url in already_done:
        return None

    try:
        response = scraper.get(anime_url)
        soup = BeautifulSoup(response.text, "html.parser")

        ep_link = soup.select_one('#DivEpisodesList div a')
        if ep_link:
            link = ep_link.get("href", "").strip()
            print(f"âœ… {link}")
            return link
        else:
            print(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© ÙÙŠ: {anime_url}")
            return None
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ {anime_url}: {e}")
        return None

# ===== Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =====
if __name__ == "__main__":
    start_url = "https://4y.qsybd.shop/Ù‚Ø§Ø¦Ù…Ø©-Ø§Ù„Ø§Ù†Ù…ÙŠ/"

    anime_links = collect_anime_links(start_url)

    # ØªØ­Ù…ÙŠÙ„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§
    if os.path.exists("first_episodes_only.json"):
        with open("first_episodes_only.json", "r", encoding="utf-8") as f:
            already_done_links = json.load(f)
    else:
        already_done_links = []

    print(f"ğŸ”„ Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ {len(already_done_links)} Ø­Ù„Ù‚Ø© Ù…Ø­ÙÙˆØ¸Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§.\n")

    # âœ… ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª 10 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ThreadPoolExecutor
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
        futures = [executor.submit(extract_first_episode, url, already_done_links) for url in anime_links]

        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            if result and result not in already_done_links:
                already_done_links.append(result)

                # Ø­ÙØ¸ ÙÙˆØ±ÙŠ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©
                with open("first_episodes_only.json", "w", encoding="utf-8") as f:
                    json.dump(already_done_links, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(already_done_links)} Ø±ÙˆØ§Ø¨Ø· Ø­Ù„Ù‚Ø§Øª Ø£ÙˆÙ„Ù‰.")
